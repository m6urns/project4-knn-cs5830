{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A single label was found in 'y_true' and 'y_pred'.\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"y_pred contains classes not in y_true\")\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 1492\n",
    "\n",
    "# Implementation based on: https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Cleveland Heart Attack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, target_column, test_size=0.2, random_state=RANDOM_SEED):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Convert target to binary classification if not already\n",
    "    if y.nunique() > 2:\n",
    "        y = y.apply(lambda x: 1 if x > y.median() else 0)\n",
    "    \n",
    "    X = X.replace('?', np.nan).astype(float)\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    \n",
    "    if test_size > 0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    check_random_state(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X, y, k, method='univariate'):\n",
    "    if method == 'univariate':\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "    elif method == 'mutual_info':\n",
    "        selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    elif method == 'rfe':\n",
    "        selector = RFE(estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_SEED), n_features_to_select=k)\n",
    "    elif method == 'random_forest':\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)\n",
    "        rf.fit(X, y)\n",
    "        importances = rf.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        return X.columns[indices[:k]].tolist()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'univariate', 'mutual_info', 'rfe', or 'random_forest'.\")\n",
    "    \n",
    "    selector.fit(X, y)\n",
    "    return X.columns[selector.get_support()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    return LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_grid_search(X, y):\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # print(\"Best parameters:\", grid_search.best_params_)\n",
    "    # print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_balanced_accuracy(y_true, y_pred):\n",
    "    try:\n",
    "        return balanced_accuracy_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return float(y_true.iloc[0] == y_pred[0])\n",
    "\n",
    "def evaluate_model_loocv(X, y, model):\n",
    "    loo = LeaveOneOut()\n",
    "    scores = []\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        score = safe_balanced_accuracy(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "        if (len(scores) % 50 == 0) or (len(scores) == n_samples):\n",
    "            print(f\"Processed {len(scores)}/{n_samples} samples. Current average score: {np.mean(scores):.3f}\")\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_feature_selection(X, y, k, method='univariate', n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        selected_features = select_features(X_train, y_train, k, method)\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "        \n",
    "        model = perform_grid_search(X_train_selected, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        \n",
    "        print(\"Precision:\", precision_score(y_test, y_pred), \"Recall:\", recall_score(y_test, y_pred), \"F1:\", f1_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    return precision_scores, recall_scores, f1_scores, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_feature_selection_methods(X, y, methods, k_range):\n",
    "    results = {}\n",
    "    for method in methods:\n",
    "        method_results = []\n",
    "        for k in k_range:\n",
    "            _, _, f1_scores, _ = evaluate_model_with_feature_selection(X, y, k, method)\n",
    "            method_results.append(np.mean(f1_scores))\n",
    "        results[method] = method_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(train_file, test_file=None, target_column='target', test_size=0.2):\n",
    "    # Extract dataset name from the file path\n",
    "    dataset_name = os.path.splitext(os.path.basename(train_file))[0]\n",
    "    \n",
    "    if test_file:\n",
    "        X_train, y_train = load_and_preprocess_data(train_file, target_column, test_size=0)\n",
    "        X_test, y_test = load_and_preprocess_data(test_file, target_column, test_size=0)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = load_and_preprocess_data(train_file, target_column, test_size=test_size)\n",
    "    \n",
    "    # Calculate the ideal test condition using Leave-One-Out\n",
    "    best_model = perform_grid_search(X_train, y_train)\n",
    "    ideal = evaluate_model_loocv(X_train, y_train, best_model)\n",
    "    print('Ideal (Leave-One-Out): %.3f' % ideal)\n",
    "    \n",
    "    # Compare different feature selection methods\n",
    "    methods = ['univariate', 'mutual_info', 'rfe', 'random_forest']\n",
    "    k_range = range(1, len(X_train.columns) + 1)\n",
    "    results = compare_feature_selection_methods(X_train, y_train, methods, k_range)\n",
    "    \n",
    "    # Create 'plots' directory if it doesn't exist\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    \n",
    "    # Plot comparison of feature selection methods\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for method in methods:\n",
    "        plt.plot(k_range, results[method], label=method)\n",
    "    plt.axhline(y=ideal, color='r', linestyle='--', label='Ideal (Leave-One-Out)')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Mean F1 Score')\n",
    "    plt.title(f'Comparison of Feature Selection Methods - {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots/feature_selection_comparison_{dataset_name}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best method and k\n",
    "    best_method = max(results, key=lambda x: max(results[x]))\n",
    "    best_k = k_range[np.argmax(results[best_method])]\n",
    "    print(f\"Best method: {best_method}\")\n",
    "    print(f\"Best number of features: {best_k}\")\n",
    "    \n",
    "    # Evaluate with best method and k\n",
    "    precision_scores, recall_scores, f1_scores, best_features = evaluate_model_with_feature_selection(X_train, y_train, best_k, best_method)\n",
    "    \n",
    "    # Plot k-fold cross-validation results\n",
    "    folds = range(1, 11)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(folds, f1_scores, fmt='o-', label='F1 Score')\n",
    "    plt.axhline(y=ideal, color='r', linestyle='--', label='Ideal (Leave-One-Out)')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'10-Fold Cross-Validation Results - {dataset_name} (k={best_k}, method={best_method})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'plots/cross_validation_results_{dataset_name}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Mean Precision: {np.mean(precision_scores):.3f}\")\n",
    "    print(f\"Mean Recall: {np.mean(recall_scores):.3f}\")\n",
    "    print(f\"Mean F1 score: {np.mean(f1_scores):.3f}\")\n",
    "    print(f\"Best features: {best_features}\")\n",
    "    \n",
    "    # Train final model and evaluate on test set\n",
    "    X_train_selected = X_train[best_features]\n",
    "    X_test_selected = X_test[best_features]\n",
    "    \n",
    "    final_model = perform_grid_search(X_train_selected, y_train)\n",
    "    \n",
    "    print(\"\\nEvaluating Challenge Dataset\")\n",
    "    y_pred = final_model.predict(X_test_selected)\n",
    "    \n",
    "    print(\"\\nChallenge Dataset Results:\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.3f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.3f}\")\n",
    "    \n",
    "    return final_model, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed()\n",
    "    train_file = 'data/cleveland.csv'\n",
    "    test_file = 'data/cleveland-test-sample.csv'\n",
    "    target_column = 'disease'\n",
    "    \n",
    "    trained_model, selected_features = train_and_evaluate_model(train_file, test_file, target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Diabetes Data\n",
    "Sourced from CDC BRFSS 2015 https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed()\n",
    "    train_file = 'data/diabetes_sample.csv'\n",
    "    test_file = None\n",
    "    target_column = 'Diabetes_binary'\n",
    "    \n",
    "    trained_model, selected_features = train_and_evaluate_model(train_file, test_file, target_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
